{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e50bb6-b287-4c89-9bdf-dabcfee5c655",
   "metadata": {},
   "source": [
    "# Decision Theory Project. AI System for Minesweeper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8dd35-8d46-42b8-b37e-54de1a54fe78",
   "metadata": {},
   "source": [
    "##### Bohdan Tymofieienko, B. S. \n",
    "###### btymofieienko@student.fontys.nl\n",
    "##### Mikolaj Hilgert, M. A. \n",
    "###### m.hilgert@student.fontys.nl\n",
    "\n",
    "\n",
    "##### 23 June 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6da2b6-dbc2-4909-a552-499691d6605f",
   "metadata": {},
   "source": [
    "### 1. Table of Contents <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "\n",
    "* [1. Table of Contents](#chapter1)\n",
    "* [2. Revision history](#chapter2)\n",
    "* [3. Abstract](#chapter3)\n",
    "* [4. Introduction](#chapter4)\n",
    "* [5. Environment](#chapter5)\n",
    "* [6. Rewards structure](#chapter6)\n",
    "* [7. Transitional probabilities and Stochasticity](#chapter7)\n",
    "* [8. Markov Decision Process](#chapter8)\n",
    "* [9. Random Agent](#chapter9)\n",
    "* [10. Value iteration. Abstract Policy](#chapter10)\n",
    "* [11. Rational Agent](#chapter11)\n",
    "* [12. Reasoning with results. Optimizations](#chapter12)\n",
    "* [13. Conclusion](#chapter13)\n",
    "* [14. Appendix A. Reflection](#chapter14)\n",
    "* [15. Appendix B. References](#chapter15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de1c06-99f1-49cc-b184-0e8f84012d76",
   "metadata": {},
   "source": [
    "### 2. Revision history <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "| Version | Author          | Date  | Change                                       |\n",
    "|---------|-----------------|-------|----------------------------------------------|\n",
    "| 0.1     | Bohdan, Mikolaj | 18.05 | Added notebook and game |\n",
    "| 0.2  | Bohdan, Mikolaj | 11.06 | Added environment and rational agent. |\n",
    "| 1.0   | Bohdan, Mikolaj | 23.06 | Added conclusions and reasoning. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a76f4-c501-46a7-9b36-777a4c1848f0",
   "metadata": {},
   "source": [
    "### 3. Abstract <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "\n",
    "For our decision theory project we decided to build an Artificial intelligence system in form of rational agent for minesweeper game. We used a value iteration approach that created an abstract policy with which our agent wins on average 3 to 4 times more games than a random agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479f0fe-f2b8-4d72-9f72-8e1bb4044ac2",
   "metadata": {},
   "source": [
    "### 4. Introduction  <a class=\"anchor\" id=\"chapter4\"></a>\n",
    "\n",
    "We chose a minesweeper game because it is a classical game that everyone can recognise and it requires a certain cognitive process that is an interesting challenge to simulate. We as not not avid players, wanted to see how an agent would fare in a game by learning the abstract rules. Our end goal was to make a rational agent that plays on a small scale board and does better than a random agent. As a proof of concept we chose to limit a board side but the idea should expand to any N x M times board. We initially took in consideration that the approach may lead to computational problems, thus we found it optimal on 2 x 3 board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c131c",
   "metadata": {},
   "source": [
    "### 5. Environment  <a class=\"anchor\" id=\"chapter4\"></a>\n",
    "\n",
    "We built our environment entirely from scratch which was useful in hindsight as we had to have full control and understanding of the game logic, to adapt rules. One change we made to the original game to make it more \"machine-friendly\" is that we removed flags from the possible actions. In order to win an agent shall open all the non-bombs cells. The environment is customisable with the size of the board, the bomb amount as well as the used seed for bomb generation. This allows for full control of the game, as you may retain the same arrangement of bombs and cells across games by passing a fixed random-seed.\n",
    "\n",
    "Our environment consists of two classes, game which is essentially just an interface for the environment to interact  with the game and environment class that encapsulates the game logic and exposes information to an agent. \n",
    "\n",
    "As we said before through the environment agent can do steps and receive all the necessary information. Environment can also render a board to demonstrate a state of the game. Environment is also in charge of transition logic and reward policy. \n",
    "\n",
    "* Action space: all the cells on a board\n",
    "* States: board itself is a state. For example: \n",
    "   \n",
    " \n",
    "        1 1 \n",
    "        # #\n",
    "        \n",
    "        s = {((0,0), 1), ((0,1), 1)}\n",
    "        \n",
    "        # - unopened cells\n",
    "        \n",
    "We decided to model our state-action space in this way because it is a simple way to compress all the information needed per step. We chose to use a tuple because it's convinient to extract values in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e33a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randint, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65c061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "  def __init__(self, n, m, bombs, seed = None):\n",
    "    self.__seed = seed\n",
    "    self.bombsCount = bombs\n",
    "    self.status = 0\n",
    "    self.board = [[\"#\" for x in range(n)] for y in range(m)]\n",
    "    self.bombs = []\n",
    "    if seed != None:\n",
    "      self.__generateBombs()\n",
    "    \n",
    "      \n",
    "  def __generateBombs(self, x = None, y = None):\n",
    "    if self.__seed != None:\n",
    "        random.seed(self.__seed)\n",
    "    while(len(self.bombs)<self.bombsCount):\n",
    "      r_y = random.randint(0, len(self.board[0])-1)\n",
    "      r_x = random.randint(0, len(self.board)-1)\n",
    "      if self.__seed == None:\n",
    "        if r_x != x and r_y != y and (r_x,r_y) not in self.bombs:\n",
    "          self.bombs.append((r_x,r_y))\n",
    "      else:\n",
    "        if (r_x,r_y) not in self.bombs:\n",
    "          self.bombs.append((r_x,r_y))\n",
    "      \n",
    "  def checkWin(self):    \n",
    "    if sum(row.count('#') for row in self.board) == self.bombsCount:\n",
    "      self.status = 1\n",
    "  \n",
    "  def getUnopenedCells(self):      \n",
    "    moves = []\n",
    "    for i in range(0, len(self.board)):\n",
    "      for j in range(0, len(self.board[0])):\n",
    "        if self.board[i][j] == \"#\":\n",
    "          moves.append((i,j))\n",
    "    return moves\n",
    "  \n",
    "  def getBoardConfig(self, bombsList):\n",
    "    self.bombs = bombsList\n",
    "    for i in range(len(self.board)):\n",
    "      for j in range(len(self.board[0])):\n",
    "          self.openCell(i,j)\n",
    "    return self.board\n",
    "    \n",
    "\n",
    "  def openCell(self, x, y):\n",
    "    if len(self.bombs) == 0:\n",
    "      self.__generateBombs(x,y)\n",
    "    if (x,y) in self.bombs:\n",
    "      self.status = -1\n",
    "      self.board[x][y] = \"B\"\n",
    "    else:\n",
    "      #Logic of openning\n",
    "      self.__recOpen(x,y)\n",
    "      self.checkWin()\n",
    "\n",
    "    \n",
    "  def __recOpen(self, x, y):\n",
    "    adj = [(a, b) for a, b in [(x - 1, y), (x, y - 1), (x + 1, y), (x, y + 1),(x - 1, y-1), (x+1, y +1),(x + 1, y-1), (x-1, y + 1)]]\n",
    "    counter = 0\n",
    "    safeCells = []\n",
    "    for test in adj:\n",
    "        if test[0]<len(self.board) and test[1]<len(self.board[0]) and test[0] >= 0 and test[1] >= 0:\n",
    "          if test in self.bombs:\n",
    "              counter += 1\n",
    "          else:\n",
    "              safeCells.append(test)\n",
    "    if counter==0:\n",
    "      self.board[x][y] = \"_\"\n",
    "    else:\n",
    "      self.board[x][y] = str(counter)\n",
    "    if self.board[x][y] == \"_\":\n",
    "      for cell in safeCells:   \n",
    "        if self.board[cell[0]][cell[1]] == \"#\":\n",
    "          self.__recOpen(cell[0],cell[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb299a",
   "metadata": {},
   "source": [
    "### 6.  Rewards structure  <a class=\"anchor\" id=\"chapter6\"></a>\n",
    "\n",
    "1) *Winning state*, so the state with all the cells opened except bombs results in  the reward of 10\n",
    "\n",
    "In order to overweight all the negative rewards the optimal positive reward is 10. This has to do with how the Q-Value is       calculated. Since the transitional probability is included in the formula it is important that the state with the positive       reward contrasts from the losing state.  \n",
    "\n",
    "2) *Losing state*, so the state with the opened bomb results in the reward of -1\n",
    "\n",
    "Relatively large negative reward is essential part of the reward structure since it leads the agent to not make decisions that result in negative rewards as the agents **core** purpose is to maximize attained reward.\n",
    "\n",
    "3) All the other states (-0.04)\n",
    "\n",
    "An agent receives a small negative reward for non end states (win or loss) in order to motivate an agent to solve the game in the least amount of steps.  \n",
    "\n",
    "\n",
    "### 7.  Transitional probabilities and Stochasticity  <a class=\"anchor\" id=\"chapter7\"></a>\n",
    "\n",
    "After an iterative design process we understood that we are dealing with the stochastic environment since an agent may face a list of possible states it can end up at and calculate their probabilities. Initially we designed out environment in a way that every transition would give your guaranteed desired state. Meaning there is no drift. \n",
    "\n",
    "However, trying to make an agent solve any minesweeper of the certain size we realized there is a need for an abstract policy. Abstract policy implies that it was not based on fixed bombs configuration but is rather suitable for any configuration. The fact of the configuration being not disclosed until the end of the game adds stochasticity. We compensate that with the transitional policy. Consider the example below:\n",
    "            \n",
    "            Current state:\n",
    "            \n",
    "            1 1\n",
    "            # #\n",
    "\n",
    "            s = {((0,0), 1), ((0,1), 1)}\n",
    "            a = (1,0)\n",
    "\n",
    "            Possibilities:\n",
    "                s_1 = {((0,0), 1), ((0,1), 1), ((1,0), 1)}\n",
    "                s_2 = {((0,0), 1), ((0,1), 1), ((1,0), B)}\n",
    "\n",
    "            P(s_1 | s, a) = 1/2  (Reward: 10)\n",
    "            P(s_2 | s, a) = 1/2  (Reward: -1)\n",
    "            \n",
    "In fact if an agent is at state s it can end up in two different states with the probability of $\\frac{1}{2}$ each. This idea extends for more possible states. \n",
    "\n",
    "The general formula is:\n",
    "\n",
    "$$\\frac{1}{N (Derivatives(s,a))}$$\n",
    "\n",
    "*Derivatives - possible states with the given action*\n",
    "\n",
    "\n",
    "### 8.  Markov Decision Process   <a class=\"anchor\" id=\"chapter8\"></a>\n",
    "\n",
    "According to the definition, markov decision processes (mdps) model decision making in discrete, stochastic, sequential environments. The essence of the model is that a decision maker, or agent, inhabits an environment, which changes state randomly in response to action choices made by the decision maker *[M.L. Littman, 2001]*. All of the factors described hold true for our environment. First of all our environment is discrete, since every time step is modeled as a distinct state (set of opened cells and hints). The environment is stochastic as described in previous section. Next step cannot be fully determined by an agent.\n",
    "\n",
    "An environment satisfies the Markov property if its state signal compactly summarizes the past without degrading the ability to predict the future *[Mark Lee 2005]*. This is exactly the case for our environment, since every state is a compact representation of the board *(current state)*. However, the state doesn't contain any information related to past states. In other words, given an agent is in the pre-winning state *(one cell to be oppened to win the game)* it doesn't matter how the agent got to that state. All that matters is an optimal action an agent takes that gives the greatest chance to end in a winning state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de4a84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Environment:\n",
    "  def __init__(self, n, m, bombs, reward_per_step, seed = None):\n",
    "    self.__game = None\n",
    "    if seed == None:\n",
    "      self.__game = Game(n, m, bombs)\n",
    "    else:\n",
    "      self.__game = Game(n, m, bombs, seed)\n",
    "    self.__actions = [(x,y) for x in range(m) for y in range(n)]\n",
    "    self.__possibleStates = self.generateAllStates()\n",
    "    self.__initial_state = []\n",
    "    self.__state = self.__initial_state\n",
    "    self.__reward_per_step = reward_per_step\n",
    "    self.__states_id_map = dict(enumerate(self.__possibleStates))\n",
    "    \n",
    "  \n",
    "  def getStatus(self):\n",
    "    return self.__game.status\n",
    "  \n",
    "  def state_coords(self, state):\n",
    "    temp = []\n",
    "    for x in state:\n",
    "      temp.append(state[0])\n",
    "    return temp          \n",
    "    \n",
    "  def __getReward(self, state, action = None):\n",
    "    count = 0\n",
    "    for x in state:\n",
    "      if x[1] == 'B':\n",
    "        return -1\n",
    "      count += 1\n",
    "    if count == ((len(self.__game.board) * len(self.__game.board[0])) - self.__game.bombsCount):\n",
    "      return 10\n",
    "    return self.__reward_per_step\n",
    "      \n",
    "  \n",
    "  def generateAllStates(self):\n",
    "    temp = []\n",
    "    m = len(self.__game.board)\n",
    "    n = len(self.__game.board[0])\n",
    "    for x in self.__actions:\n",
    "      newgame = Game(n, m,self.__game.bombsCount)\n",
    "      board = newgame.getBoardConfig([x])\n",
    "      states = self.__generatePossibleStates([(x,y) for x in range(m) for y in range(n)],[x])\n",
    "      for s in states:\n",
    "        newstate = []\n",
    "        for e in s:\n",
    "          newstate.append((e, newgame.board[e[0]][e[1]]))\n",
    "        temp.append(newstate)\n",
    "    result = []\n",
    "    for x in temp:\n",
    "      if x not in result:\n",
    "        result.append(x)\n",
    "    return result\n",
    "    \n",
    "  def __combinations(self,iterable):\n",
    "    if len(iterable) == 0:\n",
    "        return [[]]\n",
    "    combos = []\n",
    "    for combo in self.__combinations(iterable[1:]):\n",
    "      combos += [combo,combo + [iterable[0]]]\n",
    "    return combos\n",
    "\n",
    "  def __generatePossibleStates(self,iterable, bombs):\n",
    "    temp = []\n",
    "    for set_ in self.__combinations(iterable):\n",
    "      if len(set(set_).intersection(set(bombs))) <= 1:\n",
    "        temp.append(set_)\n",
    "    return temp\n",
    "  \n",
    "  def __calculate_transition(self, action):  \n",
    "    self.__state.append((action, self.__game.board[action[0]][action[1]]))\n",
    "  \n",
    "  def getCurrentState(self):\n",
    "    return self.__state\n",
    "  \n",
    "  def removeAction(self,action):\n",
    "    self.__actions.remove(action)\n",
    "  \n",
    "  def getIdByState(self, state):\n",
    "    for key, value in self.__states_id_map.items():\n",
    "      if set(value) == set(state):\n",
    "        return key\n",
    "      \n",
    "  def getStateById(self, _id):\n",
    "      return self.__states_id_map[_id]\n",
    "  \n",
    "  def isGameOver(self):\n",
    "    if self.__game.status == 1 or self.__game.status == -1:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "    \n",
    "  def isDone(self, state):\n",
    "    count = 0\n",
    "    for x in state:\n",
    "      if x[1] == 'B':\n",
    "        return True\n",
    "      count += 1\n",
    "    if count == ((len(self.__game.board) * len(self.__game.board[0])) - 1):\n",
    "      return True\n",
    "    return False\n",
    " \n",
    "  def reset(self):\n",
    "    self.__game = Game(len(self.__game.board[0]), len(self.__game.board), self.__game.bombsCount)\n",
    "    self.__state = self.__initial_state\n",
    "    return self.__state\n",
    "  \n",
    "  def step(self, action):\n",
    "    #Open cell\n",
    "    self.__game.openCell(action[0],action[1])\n",
    "    #Step logic\n",
    "    self.__calculate_transition(action)\n",
    "    observation = self.__state\n",
    "    done = self.isDone(self.__state)\n",
    "    reward = self.__getReward(self.__state)\n",
    "    info = self.__game\n",
    "    return observation, done, reward, info\n",
    "  \n",
    "\n",
    "  def render(self, powerMode=False):\n",
    "    for line in self.__game.board:\n",
    "      print(line)\n",
    "    print(\"State: \",self.__state)\n",
    "    print(\"Status: \",self.__game.status)\n",
    "    print(\"Bombs: \",self.__game.bombsCount)\n",
    "    if powerMode:  \n",
    "      print(\"Bombs: \",self.__game.bombs)\n",
    "  \n",
    "  def get_possible_states(self):\n",
    "    return self.__possibleStates\n",
    "  \n",
    "  def get_actions(self):\n",
    "    return self.__actions\n",
    "    \n",
    "  def get_transition_prob(self, action, new_state, old_state=None):\n",
    "    if self.isDone(old_state):\n",
    "      return 0  \n",
    "    \n",
    "    #Conditions\n",
    "    length = (len(new_state) == len(old_state) + 1)\n",
    "    intersection = set(new_state).intersection(set(old_state)) == set(old_state)\n",
    "    diff = list(set(new_state) - set(old_state))\n",
    "    same_action = [item[0] for item in diff] == [action]\n",
    "    \n",
    "    if not (length and intersection and same_action):\n",
    "      return 0\n",
    "    \n",
    "    count = 0\n",
    "    for x in self.__possibleStates:\n",
    "      length = (len(x) == len(old_state) + 1)\n",
    "      intersection = set(x).intersection(set(old_state)) == set(old_state)\n",
    "      diff = list(set(x) - set(old_state))\n",
    "      same_action = [item[0] for item in diff] == [action]\n",
    "      if length and intersection and same_action:\n",
    "        count+=1\n",
    "    \n",
    "    return 1 / count\n",
    "    \n",
    "    \n",
    "    \n",
    "  def getReward(self, state, action = None):\n",
    "    return self.__getReward(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578f02a",
   "metadata": {},
   "source": [
    "### 9. Random Agent  <a class=\"anchor\" id=\"chapter9\"></a>\n",
    "\n",
    "The policy function $\\pi(s) \\to a$ is the concrete implementation of the decision process of the agent (selection of an action $a$). In the cell below, you can see the effect of an agent with a random policy choosing an arbitrary action regardless of the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6aaf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell: (0, 1), reward:  0.04\n",
      "#\t1\t#\n",
      "#\t#\t#\n",
      "#\t#\t#\n",
      "[(2, 1), (1, 2)]\n",
      "cell: (2, 1), reward: -1.00\n",
      "#\t1\t#\n",
      "#\t#\t#\n",
      "#\tB\t#\n",
      "[(2, 1), (1, 2)]\n",
      "Episode done after 2 steps. total reward:  -0.96. Result: -1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "env = Environment(3,3,2,0.04,1032)\n",
    "\n",
    "def policy_random(env):\n",
    "    # action is random choice from all actions in Action Space\n",
    "    random.seed(None)\n",
    "    cell = random.choice(env.get_actions())\n",
    "    return cell\n",
    "  \n",
    "  \n",
    "total_reward = 0.0\n",
    "done = False\n",
    "nr_steps = 0\n",
    "\n",
    "\n",
    "while not done:\n",
    "  next_cell = policy_random(env)\n",
    "  state, done, reward, info = env.step(next_cell)\n",
    "  total_reward += reward\n",
    "  nr_steps += 1\n",
    "  print('cell: {}, reward: {:5.2f}'\n",
    "          .format(next_cell, reward))\n",
    "  for row in info.board:\n",
    "    print(*row, sep=\"\\t\")\n",
    "  print(info.bombs)\n",
    "print('Episode done after {} steps. total reward: {:6.2f}. Result: {}'.format(nr_steps, total_reward, info.status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3ce56ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 100 episodes\n",
      "mean:   0.13, sigma:   0.34\n",
      "\n",
      "ep:  1, total reward:  0.00\n",
      "ep:  2, total reward:  1.00\n",
      "ep:  3, total reward:  0.00\n",
      "ep:  4, total reward:  0.00\n",
      "ep:  5, total reward:  0.00\n",
      "......\n",
      "ep: 95, total reward:  0.00\n",
      "ep: 96, total reward:  0.00\n",
      "ep: 97, total reward:  0.00\n",
      "ep: 98, total reward:  0.00\n",
      "ep: 99, total reward:  0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def run_one_episode(policy):\n",
    "    env = Environment(2,3,1,-0.04)\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_action = policy(env)\n",
    "        state, done, reward, info = env.step(next_action)\n",
    "        total_reward += reward\n",
    "    if env.getStatus() == 1:\n",
    "      return 1\n",
    "    return 0\n",
    "\n",
    "def measure_performance(policy, nr_episodes=100):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for _ in range(N):\n",
    "        episode_reward = run_one_episode(policy)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    print('mean: {:6.2f}, sigma: {:6.2f}'.format(mean(all_rewards), stdev(all_rewards)))\n",
    "    print()\n",
    "    for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "    print('......')\n",
    "    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards)-5):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "\n",
    "measure_performance(policy_random)  # in Python a function pointer is simply the name of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19ef8a",
   "metadata": {},
   "source": [
    "### 10. Value iteration. Abstract Policy. <a class=\"anchor\" id=\"chapter10\"></a>\n",
    "\n",
    "\n",
    "We chose a value iteration approach because of the clear structure and nature of the problem. \n",
    "\n",
    "In essence our end-goal is an abstract policy. We can think of it as of definitive reference for an agent with an instruction on what to do at each state. In order to extract such a policy we used a value iteration procedure. It allows us to numerically calculate the values of the states of Markov decision processes, with known transition probabilities and rewards. \n",
    "\n",
    "\n",
    "With the defined so-called Q-function we can calculate that numerical value \n",
    "\n",
    "(1) $Q(s, a) = \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma \\space U(s')]$\n",
    "\n",
    "The solution of the problem reduces to solving Bellman's equation\n",
    "\n",
    "(2) $U_{i+1}(s) = \\underset{a}{max} \\sum_{s'} P(s' \\mid s, a) \\space [ R(s, a, s') + \\gamma \\space U_i(s') ]$\n",
    "\n",
    "which can be simplified using the the Q-function definition as following\n",
    "\n",
    "(3) $U_{i+1}(s) = \\underset{a}{max} \\space Q_i(s, a)$\n",
    "\n",
    "It is also proved that Value Iteration is guaranteed to converge to the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f32bc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_initial_U(mdp):\n",
    "    U = {}\n",
    "    for s in mdp.get_possible_states():\n",
    "        U[mdp.getIdByState(s)] = mdp.getReward(s)\n",
    "    return U\n",
    "    \n",
    "def Q_Value(mdp, s, a, U):\n",
    "    Q = 0.0\n",
    "    possible_states = mdp.get_possible_states()\n",
    "    for s_p in possible_states:\n",
    "        P = mdp.get_transition_prob(a, s_p, s)\n",
    "        R = mdp.getReward(s_p, a)\n",
    "        Q += P * (R + U[mdp.getIdByState(s_p)])\n",
    "    return Q\n",
    "\n",
    "def ValueIteration(mdp, error=0.00001):\n",
    "    # from AIMA 4th edition without discount gamma \n",
    "    U_p = get_initial_U(mdp) # U_p = U'\n",
    "    delta = float('inf')\n",
    "    while delta > error:\n",
    "        U = {}\n",
    "        for s in mdp.get_possible_states():\n",
    "            U[mdp.getIdByState(s)] = U_p[mdp.getIdByState(s)]\n",
    "        print_U(U)  # to illustrate the iteration process\n",
    "        delta = 0\n",
    "        for s in mdp.get_possible_states():\n",
    "            max_a = float('-inf')\n",
    "            for a in mdp.get_actions():\n",
    "                q = Q_Value(mdp, s, a, U) \n",
    "                if q > max_a:\n",
    "                    max_a = q\n",
    "            U_p[mdp.getIdByState(s)] = max_a\n",
    "            if abs(U_p[mdp.getIdByState(s)] - U[mdp.getIdByState(s)]) > delta:\n",
    "                delta = abs(U_p[mdp.getIdByState(s)] - U[mdp.getIdByState(s)])\n",
    "    return U\n",
    "\n",
    "def print_U(U):\n",
    "    print('Utilities:')\n",
    "    for key, value in U.items():\n",
    "      print(mdp.getStateById(key), '->', value) \n",
    "    print('                   ', end = '')\n",
    "\n",
    "def print_policy(pi):\n",
    "    print('Policy:')\n",
    "    for key, value in pi.items():\n",
    "      print(mdp.getStateById(key), '->', value)\n",
    "    print('                   ', end = '')\n",
    "\n",
    "\n",
    "mdp = Environment(2,3,1,-0.04,1)\n",
    "U = ValueIteration(mdp)\n",
    "print_U(U)\n",
    "\n",
    "pi_star = {}\n",
    "for s in mdp.get_possible_states():\n",
    "    if mdp.isDone(s):\n",
    "        continue # policy is not needed in stop states\n",
    "    max_a = float('-inf')\n",
    "    argmax_a = None\n",
    "    for action in mdp.get_actions():\n",
    "        q = Q_Value(mdp, s, action, U) \n",
    "        if q > max_a:\n",
    "            max_a = q\n",
    "            argmax_a = action\n",
    "    pi_star[mdp.getIdByState(s)] = argmax_a\n",
    "\n",
    "print_policy(pi_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab28ea",
   "metadata": {},
   "source": [
    "### 11. Rational agent  <a class=\"anchor\" id=\"chapter11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ae5e9",
   "metadata": {},
   "source": [
    "Our rational agent works as following. By the rules of the game, one cannot fail at the first turn. First turn an agent picks a random cell. After that the agent looks up the current state in the reference (abstract policy) and does the action. This process is repeated until the game ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8ee9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RationalAgentPlay(env,pi_star, render = False):\n",
    "  random.seed(None)\n",
    "  cell = random.choice(env.get_actions())\n",
    "  env.step(cell)\n",
    "  if render: env.render(True) \n",
    "  while not env.isGameOver():\n",
    "    state = env.getCurrentState()\n",
    "    next_move = pi_star[env.getIdByState(state)]\n",
    "    env.step(next_move) \n",
    "    if render: env.render(True) \n",
    "  return env.getStatus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d9356",
   "metadata": {},
   "source": [
    "We gathered some statistics on our rational agent behavior. We can see that our rational agent wins with the probability of approximately 0.57 comparing to 0.13 for a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fcf0e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 100 episodes\n",
      "mean:   0.57, sigma:   0.50\n",
      "ep:  1, game result:  0.00\n",
      "ep:  2, game result:  1.00\n",
      "ep:  3, game result:  1.00\n",
      "ep:  4, game result:  0.00\n",
      "ep:  5, game result:  0.00\n",
      "......\n",
      "ep: 95, game result:  0.00\n",
      "ep: 96, game result:  0.00\n",
      "ep: 97, game result:  1.00\n",
      "ep: 98, game result:  0.00\n",
      "ep: 99, game result:  0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def run_one_episode(policy):\n",
    "    env = Environment(2,3,1,-0.04)\n",
    "    if(RationalAgentPlay(env,pi_star) == 1):\n",
    "      return 1\n",
    "    return 0\n",
    "\n",
    "def measure_performance(policy, nr_episodes=100):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for _ in range(N):\n",
    "        episode_reward = run_one_episode(policy)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    print('mean: {:6.2f}, sigma: {:6.2f}'.format(mean(all_rewards), stdev(all_rewards)))\n",
    "    for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
    "        print('ep: {:2d}, game result: {:5.2f}'.format(n, episode_reward))\n",
    "    print('......')\n",
    "    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards)-5):\n",
    "        print('ep: {:2d}, game result: {:5.2f}'.format(n, episode_reward))\n",
    "\n",
    "measure_performance(policy_random)  # in Python a function pointer is simply the name of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa04252",
   "metadata": {},
   "source": [
    "### 12. Reasoning with results. Optimizations. <a class=\"anchor\" id=\"chapter12\"></a>\n",
    "\n",
    "With the given attained results, we can see that our rational agent fares much better than pure random choice. The approach using value iteration was a suitable choice as we were able to create an abstract policy which can be adapted to any bomb and/or board size configuration. As mentioned before however, value iteration is computationally a very inefficient and expensive algorithm, due to the shear amount of possible states, especially with board of larger sizes. As that results in an exponentially larger size of potential states. This can of course be solved with adding hardware or with smarter programming. \n",
    "\n",
    "Another way to improve the performance could be to make use of parallel computing for value iteration. value iteration algorithms iterates trough all the possible actions for the state which could be parallelized and would lead to better performance.  \n",
    "\n",
    "If we had to do a similiar project we would direct our research in the field of Deep Reinforcement Learning. Based on our experience we can see that there is a need in more sophisticated algorithm. To get better results we would also take discount factor in consideration, since as an agent approaches the end off the game it should be more careful at actions it takes. The idea would be to train a neural network to approximate a Q function instead of calculating it with all the possible states. This would give a space for much larger boards. \n",
    "\n",
    "The results of the two approaches (Random vs Rational) agents can be seen below:\n",
    "\n",
    "![alt text](https://i.imgur.com/HKcJLNL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3d1cb",
   "metadata": {},
   "source": [
    "### 13. Conclusion <a class=\"anchor\" id=\"chapter13\"></a>\n",
    "\n",
    "Choice of the minesweeper game gave us a lot of potential for the design of the artificial intelligence system. By designing an abstract rules, in particular by defining a reward structure and transitional probabilities we built an agent that is capable of playing the game approximately 3-4 times better than a random agent. Although the results can be improved we think that this is a significant difference to describe the behavior of our agent as intelligent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c93c9e",
   "metadata": {},
   "source": [
    "### Appendix A. Reflections  <a class=\"anchor\" id=\"chapter14\"></a>\n",
    "\n",
    "To start with, in hindsight a problem for our first reinforcement learning project was quite complicated. As it deviated quite a bit from the examples that were available. We had a hard time understanding the correct approach to such a problem. Not only it required a lot of research but also a new mindset, since we have never had an experience of working with stochastic problems. The literature we found during the research was often written in a very complex manner that we cannot yet understand. However, we are quite happy with the results we obtained and the knowledge we acquired during this project. Clearly this gave us a bigger picture of the artificial intellingence domain. It's hard to underestimate the intensity of computations needed for AI systems. As a result we had to downscale and simplify our game. Computational problems is another interesting discovery for us as software engineering students. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86572f",
   "metadata": {},
   "source": [
    "### Appendix B. References <a class=\"anchor\" id=\"chapter15\"></a>\n",
    "\n",
    "1. Markov decision process Markov Decision Process - an overview | ScienceDirect Topics, https://www.sciencedirect.com/topics/computer-science/markov-decision-process#:~:text=Markov%20decision%20processes%20mdps,made%20by%20the%20decision%20maker. Franciszek Grabski, 2001.\n",
    "\n",
    "\n",
    "2. Reinforcement Learning 3.10 Summary, http://www.incompleteideas.net/book/ebook/node37.html . Mark Lee, 2005.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4522fc0b5433295ac682d1738029d7090a655623f56a7c6d42949f6654fea004"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
